{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b316e059-2268-4e2c-b3ec-ced3fb0f3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0884008d-c657-43ed-9711-5de606999925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad=0.0009974666172638535 loss=2.4844870567321777\n"
     ]
    }
   ],
   "source": [
    "# bigram model\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_bigram_char_convertion_maps(words):\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "    stoi['.'] = 0\n",
    "    itos = {i:s for s,i in stoi.items()}\n",
    "    return (stoi, itos)\n",
    "\n",
    "def create_bigram_xs_ys(words, stoi, itos): \n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "      chs = ['.'] + list(w) + ['.']\n",
    "      for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    return (xs, ys)\n",
    "\n",
    "def train_bigram_model(words, regularization=0.01):\n",
    "    (stoi, itos) = create_bigram_char_convertion_maps(words)\n",
    "    (xs, ys) = create_bigram_xs_ys(words, stoi, itos)\n",
    "    num = xs.nelement()\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "    tolerance = 1e-3 \n",
    "    \n",
    "    for k in range(10000):\n",
    "      xenc = F.one_hot(xs, num_classes=27).float()\n",
    "      logits = xenc @ W\n",
    "      counts = logits.exp()\n",
    "      probs = counts / counts.sum(1, keepdims=True)  \n",
    "      loss = -probs[torch.arange(num), ys].log().mean() + regularization*(W**2).mean()\n",
    "      \n",
    "      # backward pass\n",
    "      W.grad = None\n",
    "      loss.backward()\n",
    "\n",
    "      # stop when grad does not change much\n",
    "      if W.grad.norm().item() < tolerance:\n",
    "        break\n",
    "          \n",
    "      # update\n",
    "      W.data += -50 * W.grad\n",
    "\n",
    "    print(f\"grad={W.grad.norm().item()} loss={loss.item()}\")\n",
    "    return W\n",
    "\n",
    "_ = train_bigram_model(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49ba2682-bc64-4477-a438-99ba3f1914d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad=0.0009976092260330915 loss=2.1318936347961426\n"
     ]
    }
   ],
   "source": [
    "# trigram model\n",
    "\n",
    "import itertools as itt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_trigram_convertion_maps(words):\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "    stoi['.'] = 0\n",
    "    itos = {i:s for s,i in stoi.items()}\n",
    "    chars.append('.')\n",
    "    bigrams = [ch1+ch2 for ch1, ch2 in list(set(itt.product(chars, chars)))]\n",
    "    bigramtoi = {cp:i for i,cp in enumerate(bigrams)}\n",
    "    itobigram = {i:cp for cp,i in bigramtoi.items()}\n",
    "    return (stoi, bigramtoi, itos)\n",
    "\n",
    "\n",
    "def create_trigram_xs_ys(words, stoi, bigramtoi):\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "      chs = ['.'] + list(w) + ['.']\n",
    "      for (ch1, ch2, ch3) in zip(chs, chs[1:], chs[2:]):\n",
    "        trigram = (ch1+ch2, ch3)\n",
    "        ix1 = bigramtoi[trigram[0]]\n",
    "        ix2 = stoi[ch3]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    return (xs, ys)\n",
    "\n",
    "def train_trigram_model(words, regularization = 0.01):\n",
    "    (stoi, bigramtoi, _) = create_trigram_convertion_maps(words)\n",
    "    (xs, ys) = create_trigram_xs_ys(words, stoi, bigramtoi)\n",
    "\n",
    "    xenc = F.one_hot(xs, num_classes=729).float()\n",
    "    xenc.shape\n",
    "\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
    "\n",
    "    tolerance = 1e-3 \n",
    "    \n",
    "    for k in range(10000):\n",
    "        \n",
    "      # forward pass\n",
    "      logits = xenc @ W\n",
    "      counts = logits.exp()\n",
    "      probs = counts / counts.sum(1, keepdims=True)\n",
    "      loss = -probs[torch.arange(xs.nelement()), ys].log().mean() + regularization*(W**2).mean()\n",
    "      \n",
    "      # backward pass\n",
    "      W.grad = None\n",
    "      loss.backward()\n",
    "\n",
    "      # stop when grad does not change much\n",
    "      if W.grad.norm().item() < tolerance:\n",
    "        break\n",
    "      \n",
    "      # update\n",
    "      W.data += -300 * W.grad\n",
    "    \n",
    "    print(f\"grad={W.grad.norm().item()} loss={loss.item()}\")\n",
    "    return W\n",
    "\n",
    "_ = train_trigram_model(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "75454511-68a9-4d2c-ba1c-fb9735bff41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split words in training data, test data, dev data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(words, test_size=0.2, random_state=42)\n",
    "dev_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2960f0d1-3825-4396-abdb-12954e41bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "grad=0.000992407905869186 loss=2.4850637912750244\n",
      "bigram_model accuracy 2932\n",
      "-------------------------\n",
      "grad=0.000996677321381867 loss=2.1279118061065674\n",
      "trigram_model accuracy 3862\n",
      "-------------------------\n",
      "bigram test: 2962, trigram test: 3880\n"
     ]
    }
   ],
   "source": [
    "# E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "# Train the bigram and trigram models only on the training set. \n",
    "# Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "def bigram_accuracy(W, data):\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    (stoi, itos) = create_bigram_char_convertion_maps(words)\n",
    "    (xs, ys) = create_bigram_xs_ys(data, stoi, itos)\n",
    "    accuracy = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        xenc = F.one_hot(torch.tensor([x.item()]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        x2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if x2 == y.item():\n",
    "            accuracy += 1\n",
    "    return accuracy\n",
    "\n",
    "def trigram_accuracy(W, data):\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    (stoi, bigramtoi, itos) = create_trigram_convertion_maps(words)\n",
    "    (xs, ys) = create_trigram_xs_ys(data, stoi, bigramtoi)\n",
    "    accuracy = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        xenc = F.one_hot(torch.tensor([x.item()]), num_classes=729).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        x2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if x2 == y.item():\n",
    "            accuracy += 1\n",
    "    return accuracy\n",
    "\n",
    "print(\"-------------------------\")\n",
    "W_bigram = train_bigram_model(train_data)\n",
    "W_bigram_accuracy = bigram_accuracy(W_bigram, dev_data)\n",
    "print(f\"bigram_model accuracy {W_bigram_accuracy}\")\n",
    "\n",
    "print(\"-------------------------\")\n",
    "W_trigram = train_trigram_model(train_data)\n",
    "W_trigram_accuracy = trigram_accuracy(W_trigram, dev_data)\n",
    "print(f\"trigram_model accuracy {W_trigram_accuracy}\")\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(f\"bigram test: {bigram_accuracy(W_bigram, test_data)}, trigram test: {trigram_accuracy(W_trigram, test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9abfa207-48dc-414b-b12e-ddbd8f0e989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad=0.0009985571959987283 loss=1.9896376132965088\n",
      "trigram_model accuracy 4212\n"
     ]
    }
   ],
   "source": [
    "# E03: use the dev set to tune the strength of smoothing (or regularization) for\n",
    "# the trigram model - i.e. try many possibilities and see which one works best based \n",
    "# on the dev set loss. What patterns can you see in the train and dev set loss as you \n",
    "# tune this strength? Take the best setting of the smoothing and evaluate on the test \n",
    "# set once and at the end. How good of a loss do you achieve?\n",
    "\n",
    "W_trigram = train_trigram_model(dev_data, 0.000001)\n",
    "W_trigram_accuracy = trigram_accuracy(W_trigram, dev_data)\n",
    "print(f\"trigram_model accuracy {W_trigram_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "566cc920-5dc8-4fce-bca5-93c4b58b4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors \n",
    "# explicitly feels wasteful. Can you delete our use of F.one_hot in favor of \n",
    "# simply indexing into rows of W?\n",
    "\n",
    "# E05: look up and use F.cross_entropy instead. You should achieve the same result. \n",
    "# Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
