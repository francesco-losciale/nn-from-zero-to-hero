{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f9b0796-da25-4e29-a26a-d3ec9e7b4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bb053c7-fb2e-4549-bca1-5e4fdb3e06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count training set occurrences for each trigram\n",
    "t = {}\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for (ch1, ch2, ch3) in zip(chs, chs[1:], chs[2:]):\n",
    "    trigram = (ch1+ch2, ch3)\n",
    "    t[trigram] = t.get(trigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1432399a-153b-4aff-bd79-aba29bc62fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertion utils for the 'next char'. 27 elements.\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca29de43-ccc4-46ba-8f6c-98906d458bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertion utils for 'previous bigram'. 729 unique elements \n",
    "chars.append('.')\n",
    "import itertools as itt\n",
    "bigrams = [ch1+ch2 for ch1, ch2 in list(set(itt.product(chars, chars)))]\n",
    "bigramtoi = {cp:i for i,cp in enumerate(bigrams)}\n",
    "itobigram = {i:cp for cp,i in bigramtoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9d610e86-c4f6-421c-bd50-513db5676311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: we use the counts from the training set to calculate probabilities table P and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c64b9016-9fb0-41da-9d71-02fcecdcb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for (ch1, ch2, ch3) in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7e0bcfaa-ad24-4064-a907-825078e78c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum((1, 2), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16214d60-4560-4b59-8911-e89030427361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bduuwjde.\n",
      "bdianasid.\n",
      "bdulexay.\n",
      "bdo.\n",
      "bdin.\n"
     ]
    }
   ],
   "source": [
    "# We can now sample using the table P\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  ix1 = 2\n",
    "  ix2 = 4\n",
    "  out = [itos[ix1] + itos[ix2]]\n",
    "  while True:\n",
    "    p = P[ix1][ix2]\n",
    "    ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix3])\n",
    "    if itos[ix3] == '.':\n",
    "      break\n",
    "    ix1 = ix2\n",
    "    ix2 = ix3\n",
    "  print(''.join(out)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9111215b-843b-4127-9786-6e5b8c6d7137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-894872.8750)\n",
      "nll=tensor(894872.8750)\n",
      "4.563047409057617\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate loss function on the training set\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "#for w in [\"andrejq\"]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for (ch1, ch2, ch3) in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    prob = P[ix1, ix2, ix3]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n += 1\n",
    "    #print(f'{ch1}{ch2}{ch3}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72c029ae-b9c0-4832-9cd3-829e8454c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: we won't use the probabilities P anymore, we will calculate the equivalent W using an ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3faab0e4-ff45-4c37-902d-0c6431e76637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of trigrams in the form ('xy','z')\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for (ch1, ch2, ch3) in zip(chs, chs[1:], chs[2:]):\n",
    "    trigram = (ch1+ch2, ch3)\n",
    "    ix1 = bigramtoi[trigram[0]]\n",
    "    ix2 = stoi[ch3]\n",
    "    #print(trigram)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "671c9de4-52d9-4aad-9937-f3d1da69eff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 729])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=729).float()\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b5b63c80-4e40-44b6-b34b-89e00ce9e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize W with some random values\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "27191a5f-12ce-4d41-8573-67981e9c8d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0927419662475586\r"
     ]
    }
   ],
   "source": [
    "# adjusts W values until we equal the loss function result from the training set above.\n",
    "xenc = F.one_hot(xs, num_classes=729).float()\n",
    "loss = 1000\n",
    "  \n",
    "while loss >= 2.092747449874878:\n",
    "    \n",
    "  # forward pass\n",
    "  logits = xenc @ W\n",
    "  counts = logits.exp()\n",
    "  probs = counts / counts.sum(1, keepdims=True)\n",
    "  loss = -probs[torch.arange(xs.nelement()), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -300 * W.grad\n",
    "\n",
    "  print(loss.item(), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a6468b6d-d9c9-45dd-af26-ea0cb50cc984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zmaunide.\n",
      "zmilyasid.\n",
      "zma.\n",
      "zmalay.\n",
      "zmacin.\n"
     ]
    }
   ],
   "source": [
    "# sampling from NN\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  ix1 = 45\n",
    "  last_bigram = itobigram[ix1]\n",
    "  out = [last_bigram]\n",
    "    \n",
    "  while True:  \n",
    "    \n",
    "    xenc = F.one_hot(torch.tensor([ix1]), num_classes=729).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    \n",
    "    ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix2])\n",
    "    last_bigram = last_bigram[1] + itos[ix2]\n",
    "    ix1 = bigramtoi[last_bigram]\n",
    "      \n",
    "    if itos[ix2] == '.':\n",
    "      break\n",
    "\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
